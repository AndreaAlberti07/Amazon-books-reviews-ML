{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing hypothesis 1\n",
    "---\n",
    "\n",
    "**_Hypothesis_**: Reviews with longer text have higher helpfulness ratings.\n",
    "\n",
    "   - **Metric**: Correlation coefficient (e.g., Pearson's correlation) between review length and helpfulness ratings. Plot the helpfulness rate as a function of the review length.\n",
    "\n",
    "- **Missing Values**:\n",
    "\n",
    "  - `review/text`: set missing values as empty string\n",
    "  - `review/helpfulness`: remove the entire sample\n",
    "\n",
    "- **Data Transformation**:\n",
    "\n",
    "  - `review/text`: Count the number of words in each review removing punctuation and stopwords\n",
    "  - `review/helpfulness`: $helpfulness = \\frac{x}{y} \\sqrt(y)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "database = client['spark_db']\n",
    "books = database['books_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy_analyze import *\n",
    "\n",
    "# Remove the samples which have no score or helpfulness data\n",
    "pipeline_remove = {'$match':{\n",
    "                        'review/text':{'$exists':True},\n",
    "                        'N_helpful'\t:{'$exists':True, '$ne':0},\n",
    "                        'Tot_votes'\t:{'$exists':True, '$ne':0}\n",
    "                        }\n",
    "                }\n",
    "\n",
    "# Retain only the required fields\n",
    "pipeline_project = {'$project':{\n",
    "                            'review/text':1,\n",
    "                            'review/helpfulness_rate':{'$multiply':[\n",
    "                                                                {'$divide':['$N_helpful','$Tot_votes']},\n",
    "                                                                {'$sqrt':'$Tot_votes'}\n",
    "                                                                ]\n",
    "                                                       },\n",
    "                            'N_helpful':1,\n",
    "                            'Tot_votes':1,\n",
    "                            '_id':0,\n",
    "                                }\n",
    "                }\n",
    "\n",
    "books_rating = books.aggregate([pipeline_remove,pipeline_project])\n",
    "\n",
    "# Convert the cursor to a dataframe\n",
    "books_rating = pd.DataFrame(list(books_rating))\n",
    "\n",
    "books_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "    \n",
    "def get_tokenize_length(text):\n",
    "    '''This function takes a string and returns the length of a list of tokens after removing stopwords and punctuations\n",
    "    \n",
    "    # Parameters\n",
    "    Input: String\n",
    "    Output: List of tokens\n",
    "    \n",
    "    '''\n",
    "        \n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word not in stopwords and word not in string.punctuation]\n",
    "    \n",
    "    return len(words)\n",
    "\n",
    "# Tokenize the reviews\n",
    "books_rating['review_length'] = books_rating['review/text'].apply(get_tokenize_length)\n",
    "\n",
    "# Store in MongoDB\n",
    "#books_1 = database['books_hypothesis_1']\n",
    "#books_1.insert_many(books_rating.to_dict('records'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the correct collection\n",
    "books_1 = database['books_hypothesis_1']\n",
    "books_rating = pd.DataFrame(books_1.find())\n",
    "del books_rating['_id']\n",
    "del books_rating['review/text']\n",
    "\n",
    "# Filter out reviews with length less than 10\n",
    "books_rating = books_rating[books_rating['review_length'] > 10]\n",
    "books_rating = books_rating[books_rating['review/helpfulness_rate'] < 100]\n",
    "\n",
    "# Filter out reviews with N_total_votes less than 20\n",
    "# For the positive bias discovered in the hypothesis 3, we need to filter out the reviews with less than 20 votes to reduce the bias in the data\n",
    "books_rating = books_rating[books_rating['Tot_votes'] > 20]\n",
    "\n",
    "# Plot the distribution of review length with respect to helpfulness rate\n",
    "books_rating.plot(kind='scatter',x='review_length',y='review/helpfulness_rate',figsize=(15,10),title='Review Length vs Helpfulness Rate')\n",
    "\n",
    "# Analyze\n",
    "scipy_analize(books_rating['review_length'],'review/length', books_rating['review/helpfulness_rate'], 'review/helpfulness_rate', [1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **First result**\n",
    "#### In general we can see that there is a correlation between the length of the review and the helpfulness of the review, as supported by the correlation coefficient. However looking at the scatter plot, a different behavior is observed for reviews with length between 0 and 500 words. This must be further investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### We might think that the correlation is positive when the review has a reasonable length, but it is negative when the review is too long. We will divide the data into bins and analyze the correlation in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5 bins of review length\n",
    "groups = [0, 400, 750, 3000]\n",
    "books_rating['length_bin'] = pd.cut(books_rating['review_length'], bins=groups, labels = [group for group in groups[1:]])\n",
    "#books_rating.drop(books_rating.index[books_rating['length_bin'].isna().values], inplace=True)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the distribution of review length with respect to helpfulness rate\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.boxplot(x='length_bin', y='review/helpfulness_rate', data=books_rating, palette='rainbow')\n",
    "plt.title('Review Length Range vs Helpfulness Rate')\n",
    "\n",
    "import scipy\n",
    "\n",
    "for el in groups[1:]:\n",
    "    df = books_rating[books_rating['length_bin']==el]\n",
    "    corr, pval = scipy.stats.kendalltau(df['review_length'], df['review/helpfulness_rate'])\n",
    "    print(f'Group number: {el}\\nCorrelation Coefficient: {corr}\\nP-value: {pval}\\n')\n",
    "    plt.figure(figsize=(15,10))\n",
    "    df.plot(kind='scatter',x='review_length',y='review/helpfulness_rate',figsize=(15,10),title=f'Review Length vs Helpfulness Rate in Group {el}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusions**\n",
    "#### The correlation between the review length and the helpfulness changes inside the different groups. Specifically it is positive for reviews under 250 words, negative for reviews over 750 words, while it is quite neutral for the other groups. However, the correlation coefficient is low in all the groups, so we can conclude that there is no strong correlation between the length of the review and its helpfulness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
