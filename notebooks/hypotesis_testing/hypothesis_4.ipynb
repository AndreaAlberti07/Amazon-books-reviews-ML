{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypothesis 4**:  \n",
    "   The rating score is influenced by individual users, whose unique personalities and personal preferences may lead them to either overestimate or underestimate a book's quality. In addition, the Anonymous tends to overrate the books\n",
    "\n",
    "   - **Metric**: ANOVA test\n",
    "\n",
    "- **Missing Values**:\n",
    "\n",
    "  - `profileName`: Missing values are set as \"Anonymous\"\n",
    "  - `review/score`: The entire sample is removed.\n",
    "\n",
    "\n",
    "\n",
    "- **Hypotheses**:\n",
    "  - **H0**: The rating score is not related to the `profileName`, as all rating scores originate from the same distribution. If we consider the rating score of each user, they have the same mean and variance.\n",
    "\n",
    "  - **H1**: The rating score is affected by the user, meaning the rating scores of each user follow a different distribution.\n",
    "\n",
    "For the sake of consistency in this analysis, users with fewer than 20 reviews are excluded. This is because a lower number of reviews is insufficient to significantly estimate statistical measures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import f_oneway\n",
    "import scipy_analyze as sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "database = client['spark_db']\n",
    "books = database['books_joined']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the aggregation pipeline\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"profileName\": {\n",
    "                \"$ifNull\": [\"$profileName\", \"Anonymous\"]\n",
    "            },\n",
    "            \"review/score\": 1\n",
    "        }\n",
    "    },\n",
    "    {'$match': {\n",
    "        'review/score': {'$exists': True, '$ne': 0.0},\n",
    "    }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute the aggregation query and create a DataFrame\n",
    "source_df = pd.DataFrame(list(books.aggregate(pipeline)))\n",
    "source_df = source_df.drop('_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the DataFrame for the analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats = source_df.groupby('profileName').mean()\n",
    "\n",
    "\n",
    "user_stats['Std_review_score'] = source_df.groupby('profileName').std()\n",
    "user_stats['N_reviews'] = source_df['profileName'].value_counts()\n",
    "user_stats = user_stats.sort_values(by='review/score', ascending=False)\n",
    "user_stats.reset_index(inplace=True)\n",
    "user_stats = user_stats[user_stats['profileName'] != 'Anonymous']\n",
    "\n",
    "user_stats.columns = ['profileName',\n",
    "                      'Mean_review/score', 'Std_review_score', 'N_reviews']\n",
    "\n",
    "user_stats = user_stats[user_stats['N_reviews'] >= 20]\n",
    "user_stats = user_stats.sort_values(by='Std_review_score', ascending=False)\n",
    "user_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_stat = user_stats.head(20)\n",
    "df_to_show = source_df.merge(subsample_stat, on='profileName', how='inner')\n",
    "df_to_show = df_to_show[['profileName', 'review/score']]\n",
    "\n",
    "# Create a boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='profileName', y='review/score', data=df_to_show)\n",
    "plt.title('Boxplot of Review Scores by Profile Name')\n",
    "plt.ylabel('Review Score')\n",
    "plt.xlabel('Profile Name')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform one-way ANOVA.\n",
    "\n",
    "The one-way ANOVA tests the null hypothesis that two or more groups have the same population mean.  \n",
    "The groups are the profileNames.  \n",
    "Results of the test :\n",
    "- F-statistic (F-statistic):\n",
    "  The F-statistic is a test statistic generated by the ANOVA test. It measures the ratio of the variation between group means to the variation within the groups.\n",
    "  A larger F-statistic indicates a greater difference in means among the groups relative to the variation within the groups.\n",
    "  In the context of ANOVA, a larger F-statistic is more likely to lead to the rejection of the null hypothesis, suggesting that there are significant differences among the groups.\n",
    "\n",
    "  \n",
    "- P-value (p_value):\n",
    "  The p-value is a measure of the evidence against the null hypothesis (H0) in statistical hypothesis testing.\n",
    "  It represents the probability of observing the results (or more extreme results) if the null hypothesis were true.\n",
    "  In the context of ANOVA, a small p-value (typically less than your chosen significance level, e.g., 0.05) suggests that the differences in means among the groups are statistically significant.\n",
    "  Conversely, a large p-value suggests that the observed differences could have occurred by random chance, and you fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = source_df.merge(subsample_stat, on='profileName', how='inner')\n",
    "subset_df = subset_df[['profileName', 'review/score']]\n",
    "\n",
    "# Group scores by user\n",
    "user_groups = [subset_df['review/score'][subset_df['profileName'] == user]\n",
    "               for user in subset_df['profileName'].unique()]\n",
    "\n",
    "# Perform ANOVA\n",
    "f_statistic, p_value = f_oneway(*user_groups)\n",
    "\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Now let's analyze if the anonymous users give lower rating score\n",
    "The hypotheses **H0** and **H1** are the same as before, but there are only two groups: *Anonymous* and *Known*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df['profileName'] = source_df['profileName'].apply(\n",
    "    lambda x: 'Known' if x != 'Anonymous' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats = source_df.groupby('profileName').mean()\n",
    "\n",
    "\n",
    "user_stats['Std_review_score'] = source_df.groupby('profileName').std()\n",
    "user_stats.reset_index(inplace=True)\n",
    "user_stats.columns = ['profileName',\n",
    "                      'Mean_review/score', 'Std_review_score']\n",
    "\n",
    "user_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='profileName', y='review/score', data=source_df)\n",
    "plt.title('Boxplot of Review Scores by Profile Name')\n",
    "plt.ylabel('Review Score')\n",
    "plt.xlabel('Profile Name')\n",
    "plt.xticks(rotation=45)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example data (replace with your datasets)\n",
    "data_user1 = source_df[source_df['profileName']\n",
    "                       == 'Anonymous']['review/score'].to_list()\n",
    "data_user2 = source_df[source_df['profileName']\n",
    "                       != 'Known']['review/score'].to_list()\n",
    "\n",
    "# Perform KS test\n",
    "sa.scipy_analize(data_user1, 'Anonymous', data_user2, 'Known', [3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Conclusion\n",
    "The ANOVA F-statistic, which is notably high, coupled with an extremely low p-value, leads us to reject the null hypothesis (H0). Consequently, we can conclude that the rating score is significantly influenced by the user. Different users tend to provide significantly different rating scores for books.  \n",
    "\n",
    "Contrary to the initial assumption, our findings indicate that anonymous users are not more critical in their book ratings. This is supported by a low Kolmogorov-Smirnov p-value, suggesting that the distribution of ratings for anonymous users does not significantly differ from that of non-anonymous users. Thus, we find no evidence to suggest that anonymous users underrate books compared to non-anonymous users.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
