{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from HDFS to MongoDB\n",
    "---\n",
    "\n",
    "### Steps:\n",
    "- Prepare the MongoDB database and collection\n",
    "\n",
    "```bash\n",
    "# Use mongo shell to create a database (spark_db) and a collection (books)\n",
    "mongosh\n",
    "use spark_db\n",
    "db.createCollection('books')\n",
    "```\n",
    "\n",
    "- Connect to MongoDB using `pymongo`\n",
    "- Connect to HDFS and read the data using `spark.read.csv`\n",
    "- Select a subset of the Spark DataFrame to import using `sample` method\n",
    "- Transform the data into a dictionary using `to_dict` method\n",
    "- Insert the data into MongoDB using `insert_many` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "database = client['spark_db']\n",
    "books = database['books_joined']\n",
    "reviews = database['book_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/16 13:52:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Connect to HDFS\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "import pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "hypothesis_number = 'books_joined'\n",
    "# Initialize Spark Context\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\", \"5g\")\\\n",
    "    .config(\"spark.executor.memory\", \"5g\")\\\n",
    "    .config(\"spark.storage.memoryFraction\", \"0.5\")\\\n",
    "    .config(\"spark.shuffle.memoryFraction\", \"0.5\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\")\\\n",
    "    .appName(hypothesis_number).getOrCreate()\n",
    "\n",
    "\n",
    "# Define the schema\n",
    "ratings_schema = StructType([\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"Price\", FloatType(), True),\n",
    "    StructField(\"User_id\", IntegerType(), True),\n",
    "    StructField(\"profileName\", StringType(), True),\n",
    "    StructField(\"review/score\", FloatType(), True),\n",
    "    StructField(\"review/time\", IntegerType(), True),\n",
    "    StructField(\"review/summary\", StringType(), True),\n",
    "    StructField(\"review/text\", StringType(), True),\n",
    "    StructField(\"N_helpful\", IntegerType(), True),\n",
    "    StructField(\"Tot_votes\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Schema for joined data\n",
    "joined_schema = StructType([\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"authors\", StringType(), True),\n",
    "    StructField(\"publisher\", StringType(), True),\n",
    "    StructField(\"publishedDate\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"Price\", FloatType(), True),\n",
    "    StructField(\"User_id\", IntegerType(), True),\n",
    "    StructField(\"profileName\", StringType(), True),\n",
    "    StructField(\"review/score\", FloatType(), True),\n",
    "    StructField(\"review/time\", IntegerType(), True),\n",
    "    StructField(\"review/summary\", StringType(), True),\n",
    "    StructField(\"review/text\", StringType(), True),\n",
    "    StructField(\"N_helpful\", IntegerType(), True),\n",
    "    StructField(\"Tot_votes\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load the data\n",
    "df_joined = spark.read.csv(\"hdfs://localhost:9900/user/book_reviews/joined_tables\",\n",
    "                           header=True, schema=joined_schema, sep='\\t')\n",
    "spark_reviews = spark.read.csv(\n",
    "    \"hdfs://localhost:9900/user/book_reviews/books_rating_cleaned.csv\", header=True, schema=ratings_schema, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert in mongoDB a subset of the joined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/16 13:53:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: $1,265 Gold, Profitable trade set-ups from StockTwits leading traders One of the biggest secrets on Wall Street is that to become consistently profitable, you need to specialize in a distinct setup. That is, you need to know how to read the signals that can help you identify an opportunity to buy or sell. In The StockTwits Edge: 40 Actionable Trade Setups from Real Market Pros, both well-known professional masters of the market and lesser-known individual traders describe their highest probability setups to teach you about an assortment of time frame and asset class-related market methods along the way. Drawing on the wisdom of some of the top minds at StockTwits, the leading stock market social networking site, this book has something for everyone, giving you exactly what you need to come up with profitable ideas and avoid financial risk, every day. Includes key trading insights from the experts at StockTwits Explains which factors of a setup are important, and why While there are many factors involved in successful trading and investing, the ability to identify profitable situations is paramount, and The StockTwits Edge gives you everything you need to achieve that goal., ['Howard Lindzon', 'Philip Pearlman', 'Ivaylo Ivanhoff'], John Wiley & Sons, 2011-06-09, ['Business & Economics'], 75.0, , Jerry Hickel, 1.0, 1111104000, Gold Drivel, This  book  earns one star merely because it presents data about gold valuation. I see litttle value in the material presented. First, the material presented is plastic bound, a format usually reserved for notes in a college course. Second, the material is amassed with typos and grammatical errors to such a degree that it detracts from the material and limits the credibility of the author. Spell check does work on most computers. The charts and graphs are grainy and difficult to comprehend, there is no scale of reference, and has no references to corroborate. They merely agree with the text, the text says the line should point up in the corresponding graph and it does. Scattered throughout the text are quotes without sources are even relevance to the surrounding data. The organization of the material leaves something to be desired as well. Most importantly, investing should be an emotionless activity in order to make the most rational decisions with one's money. The author immediately begins the text with excitement and fear: the gold super cycle is at hand, the dollar is in a state of devaluation, and the government is going to take your money if you don't move it off shore. Granted, I agree with the author on the economic complexities of commodities and their current and future rise, but we are not quite in Germany of the 1940's. The data found in this book can be discovered in many other places, presented in far more compelling ways, and for a much cheaper price. Check out,  The Coming Collapse of the Dollar and How to Profit from it,  or look at the free information on Zeal Intelligence webpages. There is commentary from 2000, weekly postings, completely (as far as I have read) void of errors, and fantastic graphs and analysis. Do not waste your money., 14, 15\n",
      " Schema: Title, description, authors, publisher, publishedDate, categories, Price, User_id, profileName, review/score, review/time, review/summary, review/text, N_helpful, Tot_votes\n",
      "Expected: Title but found: $1,265 Gold\n",
      "CSV file: hdfs://localhost:9900/user/book_reviews/joined_tables/part-00000\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f8c6278e5c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a random subset of the big data to import\n",
    "N_to_sample = 300000\n",
    "df_sample = df_joined.sample(withReplacement = False, fraction = N_to_sample/df_joined.count(), seed = 42)\n",
    "\n",
    "# Convert to a dictionary\n",
    "df_sample_dict = df_sample.toPandas().to_dict(orient='records')\n",
    "\n",
    "# Insert into MongoDB\n",
    "books.insert_many(df_sample_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert into mongoDB a subset of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random subset of the big data to import\n",
    "N_to_sample = 300000\n",
    "df_sample = spark_reviews.sample(withReplacement = False, fraction = N_to_sample/spark_reviews.count(), seed = 42)\n",
    "\n",
    "# Convert to a dictionary\n",
    "df_sample_dict = df_sample.toPandas().to_dict(orient='records')\n",
    "\n",
    "# Insert into MongoDB\n",
    "reviews.insert_many(df_sample_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
