{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Extraction\n",
    "To provide the model with the capabilities of understanding the context and detect similar words, instead of using a simple bag of words representation, we opted for a word embedding approach. In particular, we used the Word2Vec model provided by the Gensim library. The model is trained on the train set and then used to transform the reviews in a vector representation.\n",
    "The model specification is:\n",
    "- **vector_size**: 30 and 150\n",
    "- **window**: 5\n",
    "- **min_count**: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/davide/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pymongo as pm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pm.MongoClient('mongodb://localhost:27017/')\n",
    "spark_db = client['spark_db']\n",
    "books_ratings = spark_db['book_reviews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Reshaping\n",
    "The goal is to predict the helpfulness of a review based on its content. Thus all the sample without a text or with no helpfulness votes are removed.\n",
    "To take into account both the number of helfpful votes and the total votes, a laplacian smoothing has been used. In particular the helpfulness score to predict is computed as follows:\n",
    "$$helpfulness\\_score = (\\frac{helpful\\_votes + smoothing\\_param}{total\\_votes + 2*smoothing\\_param})$$\n",
    "\n",
    "Thereby the helpfulness score is in the range [0,1], specifically the score tends to the natural ratio when the number of votes increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_remove = {'$match':{\n",
    "                        'review/score':{'$exists':True},\n",
    "                        'N_helpful'\t:{'$exists':True, '$ne':0},\n",
    "                        'Tot_votes'\t:{'$exists':True, '$ne':0}\n",
    "                        }\n",
    "    \n",
    "                }\n",
    "\n",
    "smoothing_param = 1\n",
    "\n",
    "pipeline_project = {'$project':{\n",
    "                            'review/text':1,\n",
    "                            'helpfulness_score':{'$divide':[\n",
    "                                                        {'$sum':['$N_helpful', smoothing_param]},\n",
    "                                                        {'$sum': ['$Tot_votes', smoothing_param*2]}\n",
    "                                                             ]\n",
    "                                                 },\n",
    "                            '_id':0,\n",
    "                            }\n",
    "                    }\n",
    "\n",
    "mongo_dataset = books_ratings.aggregate([pipeline_remove, pipeline_project])\n",
    "df_dataset = pd.DataFrame(list(mongo_dataset))\n",
    "arr_dataset = np.array(df_dataset)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(arr_dataset[:,0], arr_dataset[:,1], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = gensim.utils.simple_preprocess(doc)\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "X_train_w2v = [preprocess(doc) for doc in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(X_train_w2v, vector_size=150, window=5, min_count=2)\n",
    "\n",
    "model.save('_gitignore/word2vec.model')\n",
    "\n",
    "def get_embedding(doc):\n",
    "    embeddings = []\n",
    "    words = preprocess(doc)\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "X_train_embedding = [get_embedding(doc) for doc in X_train]\n",
    "X_test_embedding = [get_embedding(doc) for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the embedding\n",
    "np.savez('_gitignore/train_data_wv2_150_5.npz',x = X_train_embedding, y = Y_train)\n",
    "np.savez('_gitignore/test_data_w2v_150_5.npz',x = X_test_embedding, y = Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
