{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpfulness Prediction\n",
    "## Data Science and Big Data Analytics Project\n",
    "\n",
    "---\n",
    "\n",
    "### Authors: \n",
    "- **Andrea Alberti** ([GitHub](https://github.com/AndreaAlberti07))\n",
    "- **Davide Ligari** ([GitHub](https://github.com/DavideLigari01))\n",
    "- **Cristian Andreoli** ([GitHub](https://github.com/CristianAndreoli94))\n",
    "\n",
    "### Date: September 2023\n",
    "\n",
    "---\n",
    "\n",
    "## Data: \n",
    "The chosen dataset is [Amazon Books Reviews](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews).\n",
    "\n",
    "\n",
    "## Goal:\n",
    "Build a model able to predict the helpfulness of a review based on its content. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andreaalberti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pymongo as pm\n",
    "import pyspark as ps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pm.MongoClient('mongodb://localhost:27017/')\n",
    "spark_db = client['spark_db']\n",
    "books_ratings = spark_db['books_rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Reshaping\n",
    "The goal is to predict the helpfulness of a review based on its content. Thus all the sample without a text or with no helpfulness votes are removed.\n",
    "To take into account both the number of helfpful votes and the total votes, a laplacian smoothing has been used. In particular the helpfulness score to predict is computed as follows:\n",
    "$$helpfulness\\_score = (\\frac{helpful\\_votes + smoothing\\_param}{total\\_votes + 2*smoothing\\_param})$$\n",
    "\n",
    "Thereby the helpfulness score is in the range [0,1], specifically the score tends to the natural ratio when the number of votes increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_remove = {'$match':{\n",
    "                        'review/score':{'$exists':True},\n",
    "                        'N_helpful'\t:{'$exists':True, '$ne':0},\n",
    "                        'Tot_votes'\t:{'$exists':True, '$ne':0}\n",
    "                        }\n",
    "    \n",
    "                }\n",
    "\n",
    "smoothing_param = 1\n",
    "\n",
    "pipeline_project = {'$project':{\n",
    "                            'review/text':1,\n",
    "                            'helpfulness_score':{'$divide':[\n",
    "                                                        {'$sum':['$N_helpful', smoothing_param]},\n",
    "                                                        {'$sum': ['$Tot_votes', smoothing_param*2]}\n",
    "                                                             ]\n",
    "                                                 },\n",
    "                            '_id':0,\n",
    "                            }\n",
    "                    }\n",
    "\n",
    "mongo_dataset = books_ratings.aggregate([pipeline_remove, pipeline_project])\n",
    "df_dataset = pd.DataFrame(list(mongo_dataset))\n",
    "arr_dataset = np.array(df_dataset)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(arr_dataset[:,0], arr_dataset[:,1], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Features Extraction\n",
    "To provide the model with the capabilities of understanding the context and detect similar words, instead of using a simple bag of words representation, we opted for a word embedding approach. In particular, we used the Word2Vec model provided by the Gensim library. The model is trained on the train set and then used to transform the reviews in a vector representation.\n",
    "The model specification is:\n",
    "- **vector_size**: 30\n",
    "- **window**: 5\n",
    "- **min_count**: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = gensim.utils.simple_preprocess(doc)\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "X_train_w2v = [preprocess(doc) for doc in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(X_train_w2v, vector_size=30, window=5, min_count=2)\n",
    "\n",
    "model.save('../model/_gitignore/word2vec.model')\n",
    "\n",
    "def get_embedding(doc):\n",
    "    embeddings = []\n",
    "    words = preprocess(doc)\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "X_train_embedding = [get_embedding(doc) for doc in X_train]\n",
    "X_test_embedding = [get_embedding(doc) for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the embedding\n",
    "np.savez('../model/train_data_wv2_30_5.npz',x = X_train_embedding, y = Y_train)\n",
    "np.savez('../model/test_data_w2v_30_5.npz',x = X_test_embedding, y = Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training and Evaluation using Grid Search Cross Validation\n",
    "\n",
    "The features are extracted from the review text using Word2Vec with window = 5, creating vectors of 30 features for each word. The document is the average of the vectors of the words that compose it. The model used is a Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 30, 'min_samples_split': 4, 'n_estimators': 200}\n",
      "Mean Squared Error on Test Set: 0.024280057489063204\n",
      "Mean Squared Error on Train Set: 0.004935147918521729\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = np.load('../model/_gitignore/train_data_wv2_30_5.npz', allow_pickle=True)\n",
    "test_data = np.load('../model/_gitignore/test_data_w2v_30_5.npz', allow_pickle=True)\n",
    "X_train_embedding = train_data['x']\n",
    "Y_train = train_data['y']\n",
    "X_test_embedding = test_data['x']\n",
    "Y_test = test_data['y']\n",
    "\n",
    "# Define the hyperparameter grid you want to search over\n",
    "param_grid_1 = {\n",
    "    'n_estimators': [100,200],       # Number of trees in the forest\n",
    "    'max_depth': [30,50],     # Maximum depth of the trees\n",
    "    'min_samples_split': [4,8],    # Minimum samples required to split a node\n",
    "    # Add more hyperparameters and their values to explore here\n",
    "}\n",
    "\n",
    "param_grid_2 = {\n",
    "    'n_estimators': [50, 100],       # Number of trees in the forest\n",
    "    'max_depth': [None, 10],     # Maximum depth of the trees\n",
    "    'min_samples_split': [4, 8],    # Minimum samples required to split a node\n",
    "    # Add more hyperparameters and their values to explore here\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_3 = {\n",
    "    'n_estimators': [50, 100],       # Number of trees in the forest\n",
    "    'max_depth': [None, 10],     # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5],    # Minimum samples required to split a node\n",
    "    # Add more hyperparameters and their values to explore here\n",
    "}\n",
    "\n",
    "# Create the RandomForestRegressor model\n",
    "rand_forest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object with the model and hyperparameter grid\n",
    "grid_search = GridSearchCV(estimator=rand_forest, param_grid=param_grid_1, cv=2, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to your training data\n",
    "grid_search.fit(X_train_embedding, Y_train)\n",
    "\n",
    "# Get the best hyperparameters and the best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Save the best model to a file\n",
    "joblib.dump(best_estimator, f'../model/_gitignore/rand_forest_model_md_mss_ne.gz', compress=('gzip', 3))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "Y_test_pred = best_estimator.predict(X_test_embedding)\n",
    "mse = mean_squared_error(Y_test, Y_test_pred)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "\n",
    "Y_train_pred = best_estimator.predict(X_train_embedding)\n",
    "mse = mean_squared_error(Y_train, Y_train_pred)\n",
    "print(\"Mean Squared Error on Train Set:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Use Total votes as feature to predict helpful votes\n",
    "\n",
    "### Not useful since Total votes are not available at the time of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_remove = {'$match':{\n",
    "                        'review/score':{'$exists':True},\n",
    "                        'N_helpful'\t:{'$exists':True, '$ne':0},\n",
    "                        'Tot_votes'\t:{'$exists':True, '$ne':0}\n",
    "                        }\n",
    "    \n",
    "                }\n",
    "\n",
    "smoothing_param = 1\n",
    "\n",
    "pipeline_project = {'$project':{\n",
    "                            'review/text':1,\n",
    "                            'Tot_votes':1,\n",
    "                            'N_helpful':1,\n",
    "                            '_id':0,\n",
    "                            }\n",
    "                    }\n",
    "\n",
    "mongo_dataset = books_ratings.aggregate([pipeline_remove, pipeline_project])\n",
    "df_dataset = pd.DataFrame(list(mongo_dataset))\n",
    "arr_dataset = np.array(df_dataset)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(arr_dataset[:,0:2], arr_dataset[:,2], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = gensim.utils.simple_preprocess(doc)\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "X_train_w2v = [preprocess(doc[0]) for doc in X_train[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(X_train_w2v, vector_size=30, window=5, min_count=2)\n",
    "\n",
    "def get_embedding(doc):\n",
    "    embeddings = []\n",
    "    words = preprocess(doc[0])\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(np.append(model.wv[word], doc[1]))\n",
    "    if len(embeddings) > 0:\n",
    "         return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size+1)\n",
    "\n",
    "X_train_embedding = [get_embedding(doc) for doc in X_train[:]]\n",
    "X_test_embedding = [get_embedding(doc) for doc in X_test[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('../model/train_data_wv2_totvotes_30_5.npz',x = X_train_embedding, y = Y_train)\n",
    "np.savez('../model/test_data_w2v_totvotes_30_5.npz',x = X_test_embedding, y = Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rand_forest.fit(X_train_embedding, Y_train)\n",
    "joblib.dump(rand_forest, '../model/rand_forest_model_totvotes.gz', compress=('gzip', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = rand_forest.predict(X_test_embedding)\n",
    "Y_train_pred = rand_forest.predict(X_train_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(Y_test, Y_pred):\n",
    "    return np.sqrt(sk.metrics.mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "rmse_test = rmse(Y_test, Y_test_pred)\n",
    "print('RMSE with smoothing: ', rmse_test)\n",
    "\n",
    "rmse_train = rmse(Y_train, Y_train_pred)\n",
    "print('RMSE with smoothing: ', rmse_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
