{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpfulness Prediction\n",
    "## Data Science and Big Data Analytics Project\n",
    "\n",
    "---\n",
    "\n",
    "### Authors: \n",
    "- **Andrea Alberti** ([GitHub](https://github.com/AndreaAlberti07))\n",
    "- **Davide Ligari** ([GitHub](https://github.com/DavideLigari01))\n",
    "- **Cristian Andreoli** ([GitHub](https://github.com/CristianAndreoli94))\n",
    "\n",
    "### Date: September 2023\n",
    "\n",
    "---\n",
    "\n",
    "## Data: \n",
    "The chosen dataset is [Amazon Books Reviews](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews).\n",
    "\n",
    "\n",
    "## Goal:\n",
    "Build a model able to predict the helpfulness of a review based on its content. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andreaalberti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pymongo as pm\n",
    "import pyspark as ps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pm.MongoClient('mongodb://localhost:27017/')\n",
    "spark_db = client['spark_db']\n",
    "books_ratings = spark_db['books_rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_remove = {'$match':{\n",
    "                        'review/score':{'$exists':True},\n",
    "                        'N_helpful'\t:{'$exists':True, '$ne':0},\n",
    "                        'Tot_votes'\t:{'$exists':True, '$ne':0}\n",
    "                        }\n",
    "    \n",
    "                }\n",
    "\n",
    "smoothing_param = 1\n",
    "\n",
    "pipeline_project = {'$project':{\n",
    "                            'review/text':1,\n",
    "                            'helpfulness_score':{'$divide':[\n",
    "                                                        {'$sum':['$N_helpful', smoothing_param]},\n",
    "                                                        {'$sum': ['$Tot_votes', smoothing_param*2]}\n",
    "                                                             ]\n",
    "                                                 },\n",
    "                            '_id':0,\n",
    "                            }\n",
    "                    }\n",
    "\n",
    "mongo_dataset = books_ratings.aggregate([pipeline_remove, pipeline_project])\n",
    "df_dataset = pd.DataFrame(list(mongo_dataset))\n",
    "arr_dataset = np.array(df_dataset)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(arr_dataset[:,0], arr_dataset[:,1], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = gensim.utils.simple_preprocess(doc)\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "X_train_w2v = [preprocess(doc) for doc in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(X_train_w2v, vector_size=30, window=5, min_count=2)\n",
    "\n",
    "def get_embedding(doc):\n",
    "    embeddings = []\n",
    "    words = preprocess(doc)\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "X_train_embedding = [get_embedding(doc) for doc in X_train]\n",
    "X_test_embedding = [get_embedding(doc) for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('../model/train_data_wv2_30_5.npz',x = X_train_embedding, y = Y_train)\n",
    "np.savez('../model/test_data_w2v_30_5.npz',x = X_test_embedding, y = Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rand_forest.fit(X_train_embedding, Y_train)\n",
    "joblib.dump(rand_forest, '../model/rand_forest_model.gz', compress=('gzip', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_smoothing = rand_forest.predict(X_test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(Y_test, Y_pred):\n",
    "    return np.sqrt(sk.metrics.mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "rmse_smoothing = rmse(Y_test, Y_pred_smoothing)\n",
    "print('RMSE with smoothing: ', rmse_smoothing)\n",
    "\n",
    "# Choose a dynamic threshold to determine whether a review is helpful or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other approach: change the helpfulness before, and just fit a binary classification model.\n",
    "\n",
    "# Other: use y as features and x as target, and fit a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def equation(c, x, y):\n",
    "    return (x + c) / (y + 2 * c)\n",
    "\n",
    "def find_xy_pairs(c, target_ratio, max_value=100):\n",
    "    valid_pairs = []\n",
    "\n",
    "    for x in range(1, max_value + 1):\n",
    "        for y in range(1, max_value + 1):\n",
    "            if (\n",
    "                equation(c, x, y) >= target_ratio * 0.95\n",
    "                and equation(c, x, y) <= target_ratio * 1.05\n",
    "            ):\n",
    "                valid_pairs.append((x, y))\n",
    "\n",
    "    return valid_pairs\n",
    "\n",
    "c = 5  # Replace with your desired constant value\n",
    "target_ratio = 0.8  # Replace with your desired target ratio\n",
    "max_value = 100  # You can adjust this maximum value as needed\n",
    "\n",
    "valid_pairs = find_xy_pairs(c, target_ratio, max_value)\n",
    "\n",
    "x_values = [x for x, _ in valid_pairs]\n",
    "y_values = [y for _, y in valid_pairs]\n",
    "\n",
    "plt.scatter(x_values, y_values, label=f\"(x, y) for ~{target_ratio}\", color='blue')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(f\"Pairs of (x, y) for (x+c)/(y+2c) ~ {target_ratio} (c = {c})\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Use Total votes as feature to predict helpful votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_remove = {'$match':{\n",
    "                        'review/score':{'$exists':True},\n",
    "                        'N_helpful'\t:{'$exists':True, '$ne':0},\n",
    "                        'Tot_votes'\t:{'$exists':True, '$ne':0}\n",
    "                        }\n",
    "    \n",
    "                }\n",
    "\n",
    "smoothing_param = 1\n",
    "\n",
    "pipeline_project = {'$project':{\n",
    "                            'review/text':1,\n",
    "                            'Tot_votes':1,\n",
    "                            'N_helpful':1,\n",
    "                            '_id':0,\n",
    "                            }\n",
    "                    }\n",
    "\n",
    "mongo_dataset = books_ratings.aggregate([pipeline_remove, pipeline_project])\n",
    "df_dataset = pd.DataFrame(list(mongo_dataset))\n",
    "arr_dataset = np.array(df_dataset)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(arr_dataset[:,0:2], arr_dataset[:,2], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = gensim.utils.simple_preprocess(doc)\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "X_train_w2v = [preprocess(doc[0]) for doc in X_train[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(X_train_w2v, vector_size=30, window=5, min_count=2)\n",
    "\n",
    "def get_embedding(doc):\n",
    "    embeddings = []\n",
    "    words = preprocess(doc[0])\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(np.append(model.wv[word], doc[1]))\n",
    "    if len(embeddings) > 0:\n",
    "         return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size+1)\n",
    "\n",
    "X_train_embedding = [get_embedding(doc) for doc in X_train[:]]\n",
    "X_test_embedding = [get_embedding(doc) for doc in X_test[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('../model/train_data_wv2_totvotes_30_5.npz',x = X_train_embedding, y = Y_train)\n",
    "np.savez('../model/test_data_w2v_totvotes_30_5.npz',x = X_test_embedding, y = Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/rand_forest_model_totvotes.gz']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rand_forest.fit(X_train_embedding, Y_train)\n",
    "joblib.dump(rand_forest, '../model/rand_forest_model_totvotes.gz', compress=('gzip', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = rand_forest.predict(X_test_embedding)\n",
    "Y_train_pred = rand_forest.predict(X_train_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE with smoothing:  37.83598524834085\n",
      "RMSE with smoothing:  3.6201110196690367\n"
     ]
    }
   ],
   "source": [
    "def rmse(Y_test, Y_pred):\n",
    "    return np.sqrt(sk.metrics.mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "rmse_test = rmse(Y_test, Y_test_pred)\n",
    "print('RMSE with smoothing: ', rmse_test)\n",
    "\n",
    "rmse_train = rmse(Y_train, Y_train_pred)\n",
    "print('RMSE with smoothing: ', rmse_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
