{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpfulness Prediction With Random Forest\n",
    "## Data Science and Big Data Analytics Project\n",
    "\n",
    "---\n",
    "\n",
    "### Authors: \n",
    "- **Andrea Alberti** ([GitHub](https://github.com/AndreaAlberti07))\n",
    "- **Davide Ligari** ([GitHub](https://github.com/DavideLigari01))\n",
    "- **Cristian Andreoli** ([GitHub](https://github.com/CristianAndreoli94))\n",
    "\n",
    "### Date: September 2023\n",
    "\n",
    "---\n",
    "\n",
    "## Data: \n",
    "The chosen dataset is [Amazon Books Reviews](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews).\n",
    "\n",
    "\n",
    "## Goal:\n",
    "Build a model able to predict the helpfulness of a review based on its content. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation using Grid Search Cross Validation\n",
    "\n",
    "The features are extracted from the review text using Word2Vec with window = 5, creating vectors of 30 features for each word. The document is the average of the vectors of the words that compose it. The model used is a Random Forest Classifier.\n",
    "\n",
    "| N_grid | n_estimators | max_depth | min_samples_split | Test MSE             | Test $R^2$         |\n",
    "| ------ | ------------ | --------- | ----------------- | -------------------- | ------------------ |\n",
    "| 2      | 100          | None      | 4                 | 0.025927002564121476 | 0.2581554095121458 |\n",
    "| 3      | 200          | None      | 2                 | 0.025702599368344427 | 0.2531769186466868 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pymongo as pm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Data Loading\n",
    "client = pm.MongoClient('mongodb://localhost:27017/')\n",
    "spark_db = client['spark_db']\n",
    "books_ratings = spark_db['book_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Mean Squared Error on Test Set: 0.02587508853631114\n",
      "Mean Squared Error on Train Set: 0.0036453911772139323\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the training and test data\n",
    "train_data = np.load(\n",
    "    '../_gitignore/train_data_wv2_30_5.npz', allow_pickle=True)\n",
    "test_data = np.load(\n",
    "    '../_gitignore/test_data_w2v_30_5.npz', allow_pickle=True)\n",
    "X_train_embedding = train_data['x']\n",
    "Y_train = train_data['y']\n",
    "X_test_embedding = test_data['x']\n",
    "Y_test = test_data['y']\n",
    "\n",
    "# Define the hyperparameter grid you want to search over\n",
    "param_grid_1 = {\n",
    "    'n_estimators': [100, 200],       # Number of trees in the forest\n",
    "    'max_depth': [30, 50],     # Maximum depth of the trees\n",
    "    'min_samples_split': [4, 8],    # Minimum samples required to split a node\n",
    "    # Add more hyperparameters and their values to explore here\n",
    "}\n",
    "\n",
    "param_grid_2 = {\n",
    "    'n_estimators': [50, 100],       # Number of trees in the forest\n",
    "    'max_depth': [None, 10],     # Maximum depth of the trees\n",
    "    'min_samples_split': [4, 8],    # Minimum samples required to split a node\n",
    "    # Add more hyperparameters and their values to explore here\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_3 = {\n",
    "    'n_estimators': [50, 100,200],       # Number of trees in the forest\n",
    "    'max_depth': [None, 10],     # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5],    # Minimum samples required to split a node\n",
    "    # Add more hyperparameters and their values to explore here\n",
    "}\n",
    "# Create the RandomForestRegressor model\n",
    "rand_forest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object with the model and hyperparameter grid\n",
    "grid_search = GridSearchCV(estimator=rand_forest,\n",
    "                           param_grid=param_grid_3, cv=2, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to your training data\n",
    "grid_search.fit(X_train_embedding, Y_train)\n",
    "\n",
    "# Get the best hyperparameters and the best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Save the best model to a file\n",
    "joblib.dump(best_estimator,\n",
    "            f'../trained/rand_forest_model_md_mss_ne.gz', compress=('gzip', 3))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "Y_test_pred = best_estimator.predict(X_test_embedding)\n",
    "mse = mean_squared_error(Y_test, Y_test_pred)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "\n",
    "Y_train_pred = best_estimator.predict(X_train_embedding)\n",
    "mse = mean_squared_error(Y_train, Y_train_pred)\n",
    "print(\"Mean Squared Error on Train Set:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mdNone_mss4_ne100\n",
      "R2 Score on Test Set: 0.2531769186466868\n",
      "R2 Score on Train Set: 0.8941849152318941\n"
     ]
    }
   ],
   "source": [
    "# model_path = '../trained/rand_forest_model_mdNone_mss4_ne100.gz'\n",
    "model_path = '../trained/rand_forest_model_mdNone_mss2_ne100.gz'\n",
    "loaded_model = joblib.load(model_path)\n",
    "Y_test_pred = best_estimator.predict(X_test_embedding)\n",
    "\n",
    "r2= r2_score(Y_test, Y_test_pred)\n",
    "print('Model mdNone_mss4_ne100')\n",
    "print(\"R2 Score on Test Set:\", r2)\n",
    "\n",
    "Y_train_pred = best_estimator.predict(X_train_embedding)\n",
    "r2= r2_score(Y_train, Y_train_pred)\n",
    "print(\"R2 Score on Train Set:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Use Total votes as feature to predict helpful votes\n",
    "\n",
    "### Not useful since Total votes are not available at the time of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_remove = {'$match':{\n",
    "                        'review/score':{'$exists':True},\n",
    "                        'N_helpful'\t:{'$exists':True, '$ne':0},\n",
    "                        'Tot_votes'\t:{'$exists':True, '$ne':0}\n",
    "                        }\n",
    "    \n",
    "                }\n",
    "\n",
    "smoothing_param = 1\n",
    "\n",
    "pipeline_project = {'$project':{\n",
    "                            'review/text':1,\n",
    "                            'Tot_votes':1,\n",
    "                            'N_helpful':1,\n",
    "                            '_id':0,\n",
    "                            }\n",
    "                    }\n",
    "\n",
    "mongo_dataset = books_ratings.aggregate([pipeline_remove, pipeline_project])\n",
    "df_dataset = pd.DataFrame(list(mongo_dataset))\n",
    "arr_dataset = np.array(df_dataset)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(arr_dataset[:,0:2], arr_dataset[:,2], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = gensim.utils.simple_preprocess(doc)\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "X_train_w2v = [preprocess(doc[0]) for doc in X_train[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(X_train_w2v, vector_size=30, window=5, min_count=2)\n",
    "\n",
    "def get_embedding(doc):\n",
    "    embeddings = []\n",
    "    words = preprocess(doc[0])\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(np.append(model.wv[word], doc[1]))\n",
    "    if len(embeddings) > 0:\n",
    "         return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size+1)\n",
    "\n",
    "X_train_embedding = [get_embedding(doc) for doc in X_train[:]]\n",
    "X_test_embedding = [get_embedding(doc) for doc in X_test[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('../_gitignore/train_data_wv2_totvotes_30_5.npz',x = X_train_embedding, y = Y_train)\n",
    "np.savez('../_gitignore/test_data_w2v_totvotes_30_5.npz',x = X_test_embedding, y = Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/rand_forest_model_totvotes.gz']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rand_forest.fit(X_train_embedding, Y_train)\n",
    "joblib.dump(rand_forest, '../model/rand_forest_model_totvotes.gz', compress=('gzip', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = rand_forest.predict(X_test_embedding)\n",
    "Y_train_pred = rand_forest.predict(X_train_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE with smoothing test:  9.037798949817295\n",
      "RMSE with smoothing train:  3.023416276713991\n"
     ]
    }
   ],
   "source": [
    "def rmse(Y_test, Y_pred):\n",
    "    return np.sqrt(sk.metrics.mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "rmse_test = rmse(Y_test, Y_test_pred)\n",
    "print('RMSE with smoothing test: ', rmse_test)\n",
    "\n",
    "rmse_train = rmse(Y_train, Y_train_pred)\n",
    "print('RMSE with smoothing train: ', rmse_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
