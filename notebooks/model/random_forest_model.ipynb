{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpfulness Prediction With Random Forest\n",
    "## Data Science and Big Data Analytics Project\n",
    "\n",
    "---\n",
    "\n",
    "### Authors: \n",
    "- **Andrea Alberti** ([GitHub](https://github.com/AndreaAlberti07))\n",
    "- **Davide Ligari** ([GitHub](https://github.com/DavideLigari01))\n",
    "- **Cristian Andreoli** ([GitHub](https://github.com/CristianAndreoli94))\n",
    "\n",
    "### Date: September 2023\n",
    "\n",
    "---\n",
    "\n",
    "## Data: \n",
    "The chosen dataset is [Amazon Books Reviews](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews).\n",
    "\n",
    "\n",
    "## Goal:\n",
    "Build a model able to predict the helpfulness of a review based on its content. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation using Grid Search Cross Validation\n",
    "\n",
    "The features are extracted from the review text using Word2Vec with window = 5, creating vectors of 30 features for each word. The document is the average of the vectors of the words that compose it. The model used is a Random Forest Classifier.\n",
    "\n",
    "| N_grid | n_estimators | max_depth | min_samples_split | Test MSE             | Test $R^2$         |\n",
    "| ------ | ------------ | --------- | ----------------- | -------------------- | ------------------ |\n",
    "| 2      | 100          | None      | 4                 | 0.025927002564121476 | 0.2581554095121458 |\n",
    "| 3      | 200          | None      | 2                 | 0.025702599368344427 | 0.2531769186466868 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/davide/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pymongo as pm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the training and test data\n",
    "train_data = np.load(\n",
    "    '_gitignore/train_data_wv2_30_5.npz', allow_pickle=True)\n",
    "test_data = np.load(\n",
    "    '_gitignore/test_data_w2v_30_5.npz', allow_pickle=True)\n",
    "X_train_embedding = train_data['x']\n",
    "Y_train = train_data['y']\n",
    "X_test_embedding = test_data['x']\n",
    "Y_test = test_data['y']\n",
    "\n",
    "# Define the hyperparameter grid you want to search over\n",
    "param_grid_1 = {\n",
    "    'n_estimators': [100, 200],       # Number of trees in the forest\n",
    "    'max_depth': [30, 50],     # Maximum depth of the trees\n",
    "    'min_samples_split': [4, 8],    # Minimum samples required to split a node\n",
    "    # Add more hyperparameters and their values to explore here\n",
    "}\n",
    "\n",
    "param_grid_2 = {\n",
    "    'n_estimators': [50, 100],       # Number of trees in the forest\n",
    "    'max_depth': [None, 10],     # Maximum depth of the trees\n",
    "    'min_samples_split': [4, 8],    # Minimum samples required to split a node\n",
    "    # Add more hyperparameters and their values to explore here\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_3 = {\n",
    "    'n_estimators': [50, 100,200],       # Number of trees in the forest\n",
    "    'max_depth': [None, 10],     # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5],    # Minimum samples required to split a node\n",
    "    # Add more hyperparameters and their values to explore here\n",
    "}\n",
    "rand_forest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rand_forest,\n",
    "                           param_grid=param_grid_3, cv=2, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_embedding, Y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "joblib.dump(best_estimator,\n",
    "            f'trained/rand_forest_model_md_mss_ne.gz', compress=('gzip', 3))\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "Y_test_pred = best_estimator.predict(X_test_embedding)\n",
    "mse = mean_squared_error(Y_test, Y_test_pred)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "\n",
    "Y_train_pred = best_estimator.predict(X_train_embedding)\n",
    "mse = mean_squared_error(Y_train, Y_train_pred)\n",
    "print(\"Mean Squared Error on Train Set:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Random forest with a word2vec with a vector size of 150 instead of 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test data\n",
    "train_data = np.load(\n",
    "    '_gitignore/train_data_wv2_150_5.npz', allow_pickle=True)\n",
    "test_data = np.load(\n",
    "    '_gitignore/test_data_w2v_150_5.npz', allow_pickle=True)\n",
    "X_train_embedding = train_data['x']\n",
    "Y_train = train_data['y']\n",
    "X_test_embedding = test_data['x']\n",
    "Y_test = test_data['y']\n",
    "\n",
    "rand_forest = RandomForestRegressor(n_estimators= 200,max_depth=30,min_samples_split=4 ,random_state=42)\n",
    "\n",
    "rand_forest.fit(X_train_embedding, Y_train)\n",
    "\n",
    "joblib.dump(rand_forest,\n",
    "            f'trained/rand_forest_model_md30_mss4_ne200_150.gz', compress=('gzip', 3))\n",
    "\n",
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "Y_test_pred = rand_forest.predict(X_test_embedding)\n",
    "mse = mean_squared_error(Y_test, Y_test_pred)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Set: 0.025252578405608576\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "Y_test_pred = rand_forest.predict(X_test_embedding)\n",
    "mse = mean_squared_error(Y_test, Y_test_pred)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
