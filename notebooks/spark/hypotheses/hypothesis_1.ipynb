{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing hypothesis 1 in a Big Data context using PySpark\n",
    "---\n",
    "\n",
    "**_Hypothesis_**: Reviews with longer text have higher helpfulness ratings.\n",
    "\n",
    "   - **Metric**: Correlation coefficient (e.g., Pearson's correlation) between review length and helpfulness ratings. Plot the helpfulness rate as a function of the review length.\n",
    "\n",
    "- **Missing Values**:\n",
    "\n",
    "  - `review/text`: set missing values as empty string\n",
    "  - `review/helpfulness`: remove the entire sample\n",
    "\n",
    "- **Data Transformation**:\n",
    "\n",
    "  - `review/text`: Count the number of words in each review removing punctuation and stopwords\n",
    "  - `review/helpfulness`: $helpfulness = \\frac{x}{y} \\sqrt(y)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import findspark\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, Bucketizer, Tokenizer, StopWordsRemover\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from pyspark.sql import SparkSession\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"hypothesis_1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "ratings_schema = StructType([\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"Price\", FloatType(), True),\n",
    "    StructField(\"User_id\", IntegerType(), True),\n",
    "    StructField(\"profileName\", StringType(), True),\n",
    "    StructField(\"review/score\", FloatType(), True),\n",
    "    StructField(\"review/time\", IntegerType(), True),\n",
    "    StructField(\"review/summary\", StringType(), True),\n",
    "    StructField(\"review/text\", StringType(), True),\n",
    "    StructField(\"N_helpful\", IntegerType(), True),\n",
    "    StructField(\"Tot_votes\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load the data\n",
    "df_ratings = spark.read.csv('hdfs://localhost:9900/user/book_reviews/books_rating_cleaned.csv', header=True, schema=ratings_schema, sep='\\t')\n",
    "df_ratings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the data\n",
    "df_ratings_filtered = df_ratings.filter(df_ratings['review/text'].isNotNull())\n",
    "df_ratings_filtered = df_ratings_filtered.filter(df_ratings_filtered['Tot_votes'] > 20)\n",
    "\n",
    "# Add the helpfulness ratio column\n",
    "df_ratings_filtered = df_ratings_filtered.withColumn('helpfulness_ratio', df_ratings_filtered['N_helpful']/df_ratings_filtered['Tot_votes']*sqrt(df_ratings_filtered['Tot_votes']))\n",
    "\n",
    "# Retain only the required columns\n",
    "df_ratings_selected = df_ratings_filtered.select('review/text', 'helpfulness_ratio')\n",
    "\n",
    "# Remove punctuation\n",
    "df_ratings_selected = df_ratings_selected.withColumn('review/text', lower(regexp_replace('review/text', r'[!\"#$%&\\'()*+,-./:;<=>?@\\\\^_`{|}~]', ' ')))\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol='review/text', outputCol='words')\n",
    "df_ratings_selected = tokenizer.transform(df_ratings_selected)\n",
    "\n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='words_filtered', stopWords=stopwords)\n",
    "df_ratings_selected = remover.transform(df_ratings_selected)\n",
    "\n",
    "# Compute the length of the review\n",
    "df_ratings_selected = df_ratings_selected.withColumn('review_length', size('words_filtered'))\n",
    "\n",
    "# Compute the correlation coefficient\n",
    "vector_col = VectorAssembler(inputCols=['review_length', 'helpfulness_ratio'], outputCol='features')\n",
    "dataset = vector_col.transform(df_ratings_selected).select('features')\n",
    "corr = Correlation.corr(dataset, 'features', method='spearman').collect()[0][0].toArray()[0][1]\n",
    "print(f'The Spearman correlation coefficient is {corr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into bins\n",
    "groups = [0, 250, 500, 750, float('Inf')]\n",
    "bucketizer = Bucketizer(splits=groups, inputCol='review_length', outputCol='review_length_bins')\n",
    "df_bucketized = bucketizer.transform(df_ratings_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map the bins to the labels\n",
    "labels = {0: '250', 1: '500', 2: '750', 3: 'Inf'}\n",
    "assign_labels = udf(lambda x: labels[x], StringType())\n",
    "df_bucketized = df_bucketized.withColumn('review_length_bins', assign_labels('review_length_bins'))\n",
    "\n",
    "# Sample the data to show the results with Pandas\n",
    "df_bucketized_pandas = df_bucketized.sample(False, 0.1).toPandas()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='review_length_bins', y='helpfulness_ratio', data=df_bucketized_pandas, palette='rainbow')\n",
    "plt.title('Review Length Range vs Helpfulness Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation coefficient for each group\n",
    "for el in groups[1:]: \n",
    "    df = df_bucketized.filter(df_bucketized['review_length_bins'] == el)\n",
    "    vector_col = VectorAssembler(inputCols=['review_length', 'helpfulness_ratio'], outputCol='features')\n",
    "    dataset = vector_col.transform(df).select('features')\n",
    "    corr = Correlation.corr(dataset, 'features', method='spearman').collect()[0][0].toArray()[0][1]\n",
    "    print(f'Group: {el}\\nSpearman correlation coefficient: {corr}\\n')\n",
    "    df = df.sample(False, 0.1).toPandas()\n",
    "    plt.figure(figsize=(15,10))\n",
    "    df.plot(kind='scatter',x='review_length',y='helpfulness_ratio',figsize=(15,10),title=f'Review Length vs Helpfulness Ratio in Group {el}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
