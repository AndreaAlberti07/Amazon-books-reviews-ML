{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Testing hypothesis 2\n",
    "---\n",
    "\n",
    "**_Hypothesis_**: Reviews with more positive sentiment words receive higher helpfulness ratings.\n",
    "\n",
    "- **Metric**: Mean helpfulness ratings for number of positive and negative words.\n",
    "\n",
    "- **Model**: Multinomial Naive Bayes.\n",
    "\n",
    "- **Description**:\n",
    "\n",
    "  - Use NBC as a classifier to predict the sentiment of a review.\n",
    "  - Extract the most useful words from the classifier.\n",
    "  - Compute the mean helpfulness ratings for the most useful words.\n",
    "\n",
    "**Missing Values**:\n",
    "\n",
    "  - `review/score`: remove the entire sample\n",
    "  - `review/text`: remove the entire sample\n",
    "  - `review/helpfulness`: remove the entire sample\n",
    "\n",
    "**Data Transformation**:\n",
    "\n",
    "  - `review/score`: Assign 1 to score (4, 5), 0 to score (1, 2).\n",
    "  - `review/text`: Create the BoW for the text. Fit a MNBC and count the number of positive and negative words. Graphical Plot.\n",
    "  - `review/helpfulness`: $helpfulness = \\frac{x}{y} \\sqrt(y)$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T17:33:33.734453500Z",
     "start_time": "2023-09-22T17:33:32.511784Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import nltk\n",
    "import findspark\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, VectorAssembler\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T17:33:44.707362700Z",
     "start_time": "2023-09-22T17:33:33.739456800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize spark\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\n",
    "    'local[*]').config(\"spark.driver.memory\", \"4g\").appName(\"hypothesis_2\").getOrCreate()\n",
    "\n",
    "ratings_schema = StructType([\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"Price\", FloatType(), True),\n",
    "    StructField(\"User_id\", IntegerType(), True),\n",
    "    StructField(\"profileName\", StringType(), True),\n",
    "    StructField(\"review/score\", FloatType(), True),\n",
    "    StructField(\"review/time\", IntegerType(), True),\n",
    "    StructField(\"review/summary\", StringType(), True),\n",
    "    StructField(\"review/text\", StringType(), True),\n",
    "    StructField(\"N_helpful\", IntegerType(), True),\n",
    "    StructField(\"Tot_votes\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_ratings = spark.read.csv('hdfs://localhost:9900/user/book_reviews/books_rating_cleaned.csv',\n",
    "                            header=True, schema=ratings_schema, sep='\\t')\n",
    "df_ratings.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for the training of NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T17:33:45.872348500Z",
     "start_time": "2023-09-22T17:33:44.714891Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "df_ratings_filtered = df_ratings.filter(df_ratings['review/text'].isNotNull())\n",
    "\n",
    "df_ratings_filtered = df_ratings_filtered.filter(\n",
    "    df_ratings_filtered['review/score'] != 3)\n",
    "\n",
    "df_ratings_filtered = df_ratings_filtered.filter(\n",
    "    df_ratings_filtered['Tot_votes'] != 0)\n",
    "\n",
    "# Remove punctuation and convert to lowercase the review/text column\n",
    "df_ratings_filtered = df_ratings_filtered.withColumn('review/text', lower(regexp_replace(\n",
    "    df_ratings_filtered['review/text'], r'[!\"#$%&\\'()*+,-./:;<=>?@\\\\^_`{|}~]', ' ')))\n",
    "\n",
    "# remove words with length less than 2\n",
    "df_ratings_filtered = df_ratings_filtered.withColumn(\n",
    "    'review/text', regexp_replace('review/text', r'\\b\\w{1,2}\\b', ' '))\n",
    "\n",
    "# Add the helpfulness ratio column\n",
    "df_ratings_filtered = df_ratings_filtered.withColumn(\n",
    "    'helpfulness_ratio', df_ratings_filtered['N_helpful']/df_ratings_filtered['Tot_votes']\n",
    "    *sqrt(df_ratings_filtered['Tot_votes']))\n",
    "\n",
    "# Add the class column\n",
    "df_ratings_filtered = df_ratings_filtered.withColumn(\n",
    "    'class', when(df_ratings_filtered['review/score'] >= 4, 1).otherwise(0))\n",
    "\n",
    "# Retain only the required columns\n",
    "df_ratings_selected = df_ratings_filtered.select(\n",
    "    'review/text', 'helpfulness_ratio', 'class')\n",
    "\n",
    "# Select useful columns and handle missing values\n",
    "df = df_ratings_selected.select(\n",
    "    \"class\", \"helpfulness_ratio\", \"review/text\").na.drop()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of a multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T17:37:25.806121900Z",
     "start_time": "2023-09-22T17:33:45.876347100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"review/text\", outputCol=\"words\")\n",
    "stop_words_remover = StopWordsRemover(\n",
    "    inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# Convert words to a BoW feature vector\n",
    "vectorizer = CountVectorizer(\n",
    "    inputCol=\"filtered_words\", outputCol=\"features\", vocabSize=2000)\n",
    "\n",
    "nb = NaiveBayes(labelCol=\"class\", featuresCol=\"features\",\n",
    "                predictionCol=\"prediction\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_words_remover, vectorizer, nb])\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "nb_model = model.stages[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get most positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T17:37:27.584102500Z",
     "start_time": "2023-09-22T17:37:25.810116400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = model.stages[2].vocabulary\n",
    "\n",
    "# Calculate the difference in word probabilities between class 1 and class 0\n",
    "class_0_probs = nb_model.theta.toArray()[0]\n",
    "class_1_probs = nb_model.theta.toArray()[1]\n",
    "pos_neg_ratio = class_1_probs - class_0_probs\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"word\", StringType(), True),\n",
    "    StructField(\"pos_neg_ratio\", FloatType(), True)\n",
    "])\n",
    "\n",
    "data = [(word, float(ratio)) for word, ratio in zip(vocabulary, pos_neg_ratio)]\n",
    "results_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "sorted_results_df = results_df.orderBy(col(\"pos_neg_ratio\").desc())\n",
    "top_positive_words_df = sorted_results_df.limit(800)\n",
    "top_positive_words_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the currency of the 800 most positive words in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T17:37:28.602828500Z",
     "start_time": "2023-09-22T17:37:27.588105400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize the review/text column into words\n",
    "words_df = df.withColumn(\"words\", split(col(\"review/text\"), \"\\\\s+\"))\n",
    "words_df.show(5)\n",
    "\n",
    "top_positive_words_list = top_positive_words_df.select(\n",
    "    \"word\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "def count_words(words):\n",
    "    return len([word for word in words if word in top_positive_words_list])\n",
    "\n",
    "\n",
    "count_words_udf = udf(count_words, IntegerType())\n",
    "words_df = words_df.withColumn(\"count\", count_words_udf(col(\"words\")))\n",
    "words_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Spearman correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T17:45:49.983514500Z",
     "start_time": "2023-09-22T17:37:28.606832300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_col = VectorAssembler(\n",
    "    inputCols=['count', 'helpfulness_ratio'], outputCol='features')\n",
    "\n",
    "dataset = vector_col.transform(words_df).select('features')\n",
    "\n",
    "corr = Correlation.corr(dataset, 'features', method='spearman').collect()[\n",
    "    0][0].toArray()[0][1]\n",
    "\n",
    "print(f'The Spearman correlation coefficient is {corr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graphs, just to see the distribution of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T17:51:00.672537600Z",
     "start_time": "2023-09-22T17:45:50.001523200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_camp = words_df.sample(withReplacement=False,\n",
    "                          fraction=3000/words_df.count(), seed=42)\n",
    "\n",
    "words_df_pd = df_camp.toPandas()\n",
    "\n",
    "# scatter plot of count vs helpfulness ratio\n",
    "plt.scatter(words_df_pd['count'], words_df_pd['helpfulness_ratio'], alpha=0.2)\n",
    "plt.xlabel('Number of positive words')\n",
    "plt.ylabel('Helpful score')\n",
    "plt.show()\n",
    "\n",
    "groups = [0, 10, 20, 50, 75, 100, 200]\n",
    "words_df_pd['length_bin'] = pd.cut(words_df_pd['count'], bins=groups, labels=[\n",
    "                                   group for group in groups[1:]])\n",
    "\n",
    "# Plot the distribution of positive_words with respect to helpfulness rate\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(x='length_bin', y='helpfulness_ratio',\n",
    "            data=words_df_pd, palette='rainbow')\n",
    "\n",
    "plt.title('Review Length Range vs Helpfulness Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T17:51:01.316152Z",
     "start_time": "2023-09-22T17:51:00.662539300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
