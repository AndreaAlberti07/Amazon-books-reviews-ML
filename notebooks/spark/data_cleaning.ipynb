{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning with spark\n",
    "### in this notebook the useless columns will be removed\n",
    "\n",
    "### **PLEASE NOTE :**  \n",
    "### Since this script stores the results in hadoop, execute it only once, otherwise an error will be thrown\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "import findspark\n",
    "import string\n",
    "\n",
    "from pyspark.sql.functions import col, sum,split, regexp_replace,lit,lower\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the spark installation\n",
    "findspark.init()\n",
    "\n",
    "# Initialize a SparkContext\n",
    "spark_session = SparkSession.builder.appName(\"data_cleaning\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect and import data from HDFS directly into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for better manipulation\n",
    "\n",
    "data_schema = StructType([\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"authors\", StringType(), True),\n",
    "    StructField(\"image\", StringType(), True),\n",
    "    StructField(\"previewLink\", StringType(), True),\n",
    "    StructField(\"publisher\", StringType(), True),\n",
    "    StructField(\"publishedDate\", StringType(), True),\n",
    "    StructField(\"infoLink\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"ratingsCount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "ratings_schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), True),\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"Price\", FloatType(), True),\n",
    "    StructField(\"User_id\", IntegerType(), True),\n",
    "    StructField(\"profileName\", StringType(), True),\n",
    "    StructField(\"review/helpfulness\", StringType(), True),\n",
    "    StructField(\"review/score\", FloatType(), True),\n",
    "    StructField(\"review/time\", IntegerType(), True),\n",
    "    StructField(\"review/summary\", StringType(), True),\n",
    "    StructField(\"review/text\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load the original data\n",
    "\n",
    "df_data = spark_session.read.option('escape', '\"').csv(\n",
    "    'hdfs://localhost:9900/user/book_reviews/original_data/books_data.csv', header=True, schema=data_schema)\n",
    "df_ratings = spark_session.read.option('escape', '\"').csv(\n",
    "    'hdfs://localhost:9900/user/book_reviews/original_data/books_rating.csv', header=True, schema=ratings_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations\n",
    "---\n",
    "\n",
    "### - Remove useless columns\n",
    "\n",
    "These are the columns whcih are not useful for our analysis. The original files are kept unchanged in HDFS, and the new files are stored in HDFS as well.\n",
    "\n",
    "**Data Table:**\n",
    "All the links are removed.\n",
    "- image\n",
    "- previewLink\n",
    "- infoLink\n",
    "- ratingsCount\n",
    "\n",
    "**Rating Table:**\n",
    "- id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove image column from data\n",
    "df_data = df_data.drop(df_data.image)\n",
    "\n",
    "# Remove previewLink column from data\n",
    "df_data = df_data.drop(df_data.previewLink)\n",
    "\n",
    "# Remove infoLink column from data\n",
    "df_data = df_data.drop(df_data.infoLink)\n",
    "\n",
    "# Remove ratingsCount column from data\n",
    "df_data = df_data.drop(df_data.ratingsCount)\n",
    "\n",
    "# Show the results\n",
    "df_data.show(5)\n",
    "\n",
    "# Remove Id column from ratings data\n",
    "df_ratings = df_ratings.drop(df_ratings.Id)\n",
    "\n",
    "# Show the results\n",
    "df_ratings.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Remove all the punctuation inside each column\n",
    "\n",
    "This is to avoid parsing problem when the csv in read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove illegal characters from the review\n",
    "ratings_cols_to_change = ['Title', 'profileName',\n",
    "                          'review/summary', 'review/text']\n",
    "\n",
    "for column in ratings_cols_to_change:\n",
    "    df_ratings = df_ratings.withColumn(\n",
    "        column, regexp_replace(col(column), \"\\t\", \" \"))\n",
    "    df_ratings = df_ratings.withColumn(\n",
    "        column, regexp_replace(col(column), \"\\n\", \" \"))\n",
    "    df_ratings = df_ratings.withColumn(\n",
    "        column, regexp_replace(col(column), \"\\\"\", \" \"))\n",
    "    df_ratings = df_ratings.withColumn(\n",
    "        column, regexp_replace(col(column), \"\\\\\\\\\", \" \"))\n",
    "\n",
    "\n",
    "# Remove illegal characters from the books data\n",
    "data_cols_to_change = ['Title', 'description',\n",
    "                       'authors', 'publisher', 'categories']\n",
    "for column in data_cols_to_change:\n",
    "    df_data = df_data.withColumn(\n",
    "        column, regexp_replace(col(column), \"\\t\", \" \"))\n",
    "    df_data = df_data.withColumn(\n",
    "        column, regexp_replace(col(column), \"\\n\", \" \"))\n",
    "    df_data = df_data.withColumn(\n",
    "        column, regexp_replace(col(column), \"\\\"\", \" \"))\n",
    "    df_data = df_data.withColumn(\n",
    "        column, regexp_replace(col(column), \"\\\\\\\\\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate the helpfulness into two columns\n",
    "df_ratings = df_ratings.withColumn('N_helpful', split(col('review/helpfulness'), '/').getItem(\n",
    "    0)).withColumn('Tot_votes', split(col('review/helpfulness'), '/').getItem(1))\n",
    "\n",
    "# Remove review/helpfulness column from ratings data\n",
    "df_ratings = df_ratings.drop(col('review/helpfulness'))\n",
    "\n",
    "df_ratings.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if a given column contains a given character\n",
    "\n",
    "# contains_A = df_ratings.filter(col(\"review/text\").contains(\"\\t\")).count() > 0\n",
    "# print(\"Does the 'name' column contain 'A'? \", contains_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the results in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings.repartition(1).write.csv(\n",
    "    'hdfs://localhost:9900/user/book_reviews/books_rating_cleaned', mode='overwrite', header=True, sep='\\t')\n",
    "\n",
    "df_data.repartition(1).write.csv(\n",
    "    'hdfs://localhost:9900/user/book_reviews/books_data_cleaned', mode='overwrite', header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Read the new data to check soundness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = spark_session.read.csv(\n",
    "    'hdfs://localhost:9900/user/book_reviews/books_data_cleaned', header=True, inferSchema=True, sep='\\t')\n",
    "rating_df = spark_session.read.csv(\n",
    "    'hdfs://localhost:9900/user/book_reviews/books_rating_cleaned', header=True, inferSchema=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.show(5)\n",
    "#data_df.printSchema()\n",
    "print(\"Num values :\",data_df.count())\n",
    "#data_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.show(5)\n",
    "#rating_df.printSchema()\n",
    "print(\"Num values :\",rating_df.count())\n",
    "#rating_df.describe().show()\n",
    "\n",
    "rating_df.filter(rating_df['Title'].startswith('17 Contemporary Christian')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the Tables with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the two tables on Title\n",
    "joined_df = data_df.join(rating_df, on=['Title'], how='inner')\n",
    "\n",
    "joined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.write.csv('hdfs://localhost:9900/user/book_reviews/joined_tables_spark', mode='overwrite', header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
