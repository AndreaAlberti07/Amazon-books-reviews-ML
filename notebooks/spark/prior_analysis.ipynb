{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration with Spark\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import findspark\n",
    "\n",
    "# Locate the spark installation\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a SparkContext\n",
    "spark = SparkSession.builder.appName(\"prior_analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect and import data from HDFS directly into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Session\n",
    "#spark = ps.sql.SparkSession(sc)\n",
    "\n",
    "# Define schema for better manipulation\n",
    "\n",
    "data_schema = StructType([\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"authors\", StringType(), True),\n",
    "    StructField(\"image\", StringType(), True),\n",
    "    StructField(\"previewLink\", StringType(), True),\n",
    "    StructField(\"publisher\", StringType(), True),\n",
    "    StructField(\"publishedDate\", StringType(), True),\n",
    "    StructField(\"infoLink\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"ratingsCount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "ratings_schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), True),\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"Price\", FloatType(), True),\n",
    "    StructField(\"User_id\", IntegerType(), True),\n",
    "    StructField(\"profileName\", StringType(), True),\n",
    "    StructField(\"review/helpfulness\", StringType(), True),\n",
    "    StructField(\"review/score\", FloatType(), True),\n",
    "    StructField(\"review/time\", IntegerType(), True),\n",
    "    StructField(\"review/summary\", StringType(), True),\n",
    "    StructField(\"review/text\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Load the data\n",
    "\n",
    "df_data = spark.read.option('escape','\"').csv('hdfs://localhost:9900/user/book_reviews/original_data/books_data.csv', header=True, schema=data_schema)\n",
    "df_ratings = spark.read.option('escape','\"').csv('hdfs://localhost:9900/user/book_reviews/original_data/books_rating.csv', header=True, schema=ratings_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "- Show the first 5 rows of the data\n",
    "- Investigate the inferred schema of the data\n",
    "- Discover data dimensionality\n",
    "- Show some statistics\n",
    "- Discover null values\n",
    "- Discover the number of distinct values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data\n",
    "print('Data Table: \\n')\n",
    "df_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ratings Table: \\n')\n",
    "df_ratings.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate the schema\n",
    "print('Data Table Schema: \\n')\n",
    "df_data.printSchema()\n",
    "\n",
    "print('Ratings Table Schema: \\n')\n",
    "df_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensionality\n",
    "print(f'Data Table Dimensionality: {df_data.count(), len(df_data.columns)}')\n",
    "print(f'Ratings Table Dimensionality: {df_ratings.count(), len(df_ratings.columns)}')\n",
    "\n",
    "# Statistical summary\n",
    "print('Data Table Summary: \\n')\n",
    "df_data.describe().show()\n",
    "\n",
    "print('Ratings Table Summary: \\n')\n",
    "df_ratings.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for number of distinct values for each column in %\n",
    "n_distinct_list = []\n",
    "\n",
    "for c in df_data.columns:\n",
    "    n_distinct = df_data.select(c).distinct().count()\n",
    "    n_distinct_list.append(n_distinct)\n",
    "\n",
    "df_data_pandas = pd.DataFrame(index = df_data.columns, columns = ['N. Distinct Values'], data = (np.array((n_distinct_list))/df_data.count())*100)\n",
    "\n",
    "n_distinct_list = []\n",
    "\n",
    "for c in df_ratings.columns:\n",
    "    n_distinct = df_ratings.select(c).distinct().count()\n",
    "    n_distinct_list.append(n_distinct)\n",
    "\n",
    "df_ratings_pandas = pd.DataFrame(index = df_ratings.columns, columns = ['N. Distinct Values'], data = (np.array((n_distinct_list))/df_ratings.count())*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Check for missing values\n",
    "df_data_pandas_tmp = df_data.select([sum(col(c).isNull().cast('int')).alias(c) for c in df_data.columns]).toPandas()\n",
    "# Add to the pandas summary dataframe\n",
    "df_data_pandas['N. Missing Values'] = (df_data_pandas_tmp.loc[0, :]/df_data.count()*100).tolist()\n",
    "\n",
    "# Check for missing values\n",
    "df_ratings_pandas_tmp = df_ratings.select([sum(col(c).isNull().cast('int')).alias(c) for c in df_ratings.columns]).toPandas()\n",
    "# Add to the pandas summary dataframe\n",
    "df_ratings_pandas['N. Missing Values'] = (df_ratings_pandas_tmp.loc[0, :]/df_ratings.count()*100).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data_pandas.set_index('Column', inplace=True)\n",
    "df_data_pandas.plot(title='Data Table Summary', kind='bar', ylabel='Percentage')\n",
    "\n",
    "#df_ratings_pandas.set_index('Column', inplace=True)\n",
    "df_ratings_pandas.plot(title='Ratings Table Summary', kind='bar', ylabel='Percentage')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
