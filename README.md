## Mandatory tasks:

- [ ] You must write at least one batch MapReduce Job on Hadoop (you can also use Hive -
      in this case, you may expect questions on the form of your Mapper and Reducer
      functions)
- [ ] You must use PySpark facilities to use Spark (DataFrames or MLib) for designing a
      Final Examination Assignment
      Points to be covered in the project
      scalable solution so that your analysis can be reproduced also if your data source is a
      Big Data one
- [ ] You must use MongoDB as a project repository. Write at list one complex MongoDB query to filter your data
- [ ] You must use Python libraries for data analysis (depending on your data and on your
      analysis direction: Pandas, Scikit-learn, and/or Networkx, Seaborn, Matplotlib, and so
      forth)
