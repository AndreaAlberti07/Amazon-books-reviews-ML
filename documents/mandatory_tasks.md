## Mandatory tasks:

- [ ] Perform a preliminary data exploration analysis to understand how your data look like
      and to formulate initial hypotheses (choose your analysis direction and strategy).
- [ ] Consolidate your hypotheses and start to prepare your data for the analysis.
- [ ] Design a data analysis campaign, provide a proof to your hypotheses.
- [ ] Prepare a 10 minutes (please respect this timeline) PowerPoint (or similar tool)
      presentation for discussing your project during the exam
- [ ] You have to think also to data scalability and you should IMAGINE
      that your data source could be a Big Data Source. Therefore, you have to design a
      solution (provide an architecture) that scales in such a Big Data context. Do not make
      assumptions about input data dimension unless you are filtering them - meaning that
      you control the final size (using your Big Data tools).
- [ ] You must write at least one batch MapReduce Job on Hadoop (you can also use Hive -
      in this case, you may expect questions on the form of your Mapper and Reducer
      functions)
- [ ] You must use PySpark facilities to use Spark (DataFrames or MLib) for designing a
      Final Examination Assignment
      Points to be covered in the project
      scalable solution so that your analysis can be reproduced also if your data source is a
      Big Data one
- [ ] You must use MongoDB as a project repository. Write at list one complex MongoDB query to filter your data
- [ ] You must use Python libraries for data analysis (depending on your data and on your
      analysis direction: Pandas, Scikit-learn, and/or Networkx, Seaborn, Matplotlib, and so
      forth)
